{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/talerez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Extract QA Pairs\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    if not data['qa_pairs']:\n",
    "        return None\n",
    "    \n",
    "    qa_pairs = data['qa_pairs']\n",
    "    df = pd.DataFrame(qa_pairs)\n",
    "    #df['podcast_name'] = data['podcast_name']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering\n",
    "def create_features(df):\n",
    "    df['input_text'] = \"question: \" + df['question']\n",
    "    df['target_text'] = \"<start> \" + df['answer'] + \" <end>\"\n",
    "    return df[['input_text', 'target_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Cleaning\n",
    "def clean_text(text):\n",
    "    return text.strip().replace('\\n', ' ').replace('\\t', ' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['input_text'] = df['input_text'].apply(clean_text)\n",
    "    df['target_text'] = df['target_text'].apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(folder_path):\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = load_data(file_path)\n",
    "            \n",
    "            if df is None:\n",
    "                continue\n",
    "\n",
    "            df = create_features(df)\n",
    "            df = clean_data(df)\n",
    "            all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch custom tokenizer class to replace Keras Tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self, num_words=None, oov_token=\"<OOV>\"):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.word_counts = {}\n",
    "        self.document_count = 0\n",
    "        \n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "            for word in text.split():\n",
    "                if word in self.word_counts:\n",
    "                    self.word_counts[word] += 1\n",
    "                else:\n",
    "                    self.word_counts[word] = 1\n",
    "        \n",
    "        # Sort words by frequency\n",
    "        words_by_count = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Create word_index\n",
    "        for i, (word, _) in enumerate(words_by_count):\n",
    "            if self.num_words and i >= self.num_words:\n",
    "                break\n",
    "            self.word_index[word] = i + 1  # Reserve 0 for padding\n",
    "        \n",
    "        # Create index_word\n",
    "        self.index_word = {v: k for k, v in self.word_index.items()}\n",
    "        \n",
    "        # Add OOV token\n",
    "        if self.oov_token:\n",
    "            self.word_index[self.oov_token] = len(self.word_index) + 1\n",
    "            self.index_word[len(self.word_index)] = self.oov_token\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        result = []\n",
    "        for text in texts:\n",
    "            sequence = []\n",
    "            for word in text.split():\n",
    "                if word in self.word_index:\n",
    "                    sequence.append(self.word_index[word])\n",
    "                elif self.oov_token:\n",
    "                    sequence.append(self.word_index[self.oov_token])\n",
    "            result.append(sequence)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation for pad_sequences\n",
    "def pad_sequences(sequences, maxlen=None, padding='post', truncating='post', value=0):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:  # 'pre'\n",
    "                seq = seq[-maxlen:]\n",
    "        \n",
    "        pad_length = maxlen - len(seq)\n",
    "        if padding == 'post':\n",
    "            padded_seq = seq + [value] * pad_length\n",
    "        else:  # 'pre'\n",
    "            padded_seq = [value] * pad_length + seq\n",
    "        \n",
    "        padded_sequences.append(padded_seq)\n",
    "    \n",
    "    return np.array(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encoder_inputs, decoder_inputs, decoder_outputs):\n",
    "        self.encoder_inputs = torch.LongTensor(encoder_inputs).to(device)\n",
    "        self.decoder_inputs = torch.LongTensor(decoder_inputs).to(device)\n",
    "        self.decoder_outputs = torch.LongTensor(decoder_outputs).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'encoder_inputs': self.encoder_inputs[idx],\n",
    "            'decoder_inputs': self.decoder_inputs[idx],\n",
    "            'decoder_outputs': self.decoder_outputs[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, coverage):\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Combine coverage vector with attention\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)) + coverage)\n",
    "        attention = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "\n",
    "        # Track cumulative attention (coverage)\n",
    "        coverage += attention.unsqueeze(2)\n",
    "\n",
    "        context = torch.bmm(attention.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attention, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs, coverage):\n",
    "        embedded = self.embedding(x)\n",
    "        context, _, coverage = self.attention(hidden[-1], encoder_outputs, coverage)\n",
    "        context = context.unsqueeze(1).repeat(1, embedded.size(1), 1)\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        outputs, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        predictions = self.fc(torch.cat((outputs, context), dim=2))\n",
    "\n",
    "        return predictions, hidden, cell, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs):\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_inputs)\n",
    "        coverage = torch.zeros_like(encoder_outputs).to(device)\n",
    "        decoder_outputs, _, _, _ = self.decoder(decoder_inputs, hidden, cell, encoder_outputs, coverage)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, vocab_size, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch['encoder_inputs'], batch['decoder_inputs'])\n",
    "            \n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = batch['decoder_outputs'].view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 6.6749\n",
      "Epoch 2/50, Loss: 5.5385\n",
      "Epoch 3/50, Loss: 5.0212\n",
      "Epoch 4/50, Loss: 4.6528\n",
      "Epoch 5/50, Loss: 4.3242\n",
      "Epoch 6/50, Loss: 3.9687\n",
      "Epoch 7/50, Loss: 3.5901\n",
      "Epoch 8/50, Loss: 3.2103\n",
      "Epoch 9/50, Loss: 2.8435\n",
      "Epoch 10/50, Loss: 2.5281\n",
      "Epoch 11/50, Loss: 2.2640\n",
      "Epoch 12/50, Loss: 2.0430\n",
      "Epoch 13/50, Loss: 1.8518\n",
      "Epoch 14/50, Loss: 1.6865\n",
      "Epoch 15/50, Loss: 1.5402\n",
      "Epoch 16/50, Loss: 1.4089\n",
      "Epoch 17/50, Loss: 1.2874\n",
      "Epoch 18/50, Loss: 1.1842\n",
      "Epoch 19/50, Loss: 1.0806\n",
      "Epoch 20/50, Loss: 0.9852\n",
      "Epoch 21/50, Loss: 0.8980\n",
      "Epoch 22/50, Loss: 0.8220\n",
      "Epoch 23/50, Loss: 0.7495\n",
      "Epoch 24/50, Loss: 0.6811\n",
      "Epoch 25/50, Loss: 0.6220\n",
      "Epoch 26/50, Loss: 0.5653\n",
      "Epoch 27/50, Loss: 0.5157\n",
      "Epoch 28/50, Loss: 0.4806\n",
      "Epoch 29/50, Loss: 0.4673\n",
      "Epoch 30/50, Loss: 0.4041\n",
      "Epoch 31/50, Loss: 0.3488\n",
      "Epoch 32/50, Loss: 0.3035\n",
      "Epoch 33/50, Loss: 0.2696\n",
      "Epoch 34/50, Loss: 0.2407\n",
      "Epoch 35/50, Loss: 0.2159\n",
      "Epoch 36/50, Loss: 0.1944\n",
      "Epoch 37/50, Loss: 0.1753\n",
      "Epoch 38/50, Loss: 0.1585\n",
      "Epoch 39/50, Loss: 0.1449\n",
      "Epoch 40/50, Loss: 0.1320\n",
      "Epoch 41/50, Loss: 0.1195\n",
      "Epoch 42/50, Loss: 0.1082\n",
      "Epoch 43/50, Loss: 0.0996\n",
      "Epoch 44/50, Loss: 0.0915\n",
      "Epoch 45/50, Loss: 0.0834\n",
      "Epoch 46/50, Loss: 0.0752\n",
      "Epoch 47/50, Loss: 0.0681\n",
      "Epoch 48/50, Loss: 0.0611\n",
      "Epoch 49/50, Loss: 0.0543\n",
      "Epoch 50/50, Loss: 0.0478\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../data/qa_pairs/huberman_lab/\"  # Change this to your data path\n",
    "df = load_all_data(folder_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "special_tokens = {\"additional_special_tokens\": [\"<start>\", \"<end>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Convert text to sequences\n",
    "X = tokenizer(df['input_text'].tolist(), padding='max_length', max_length=512, truncation=True, return_tensors='pt')['input_ids']\n",
    "              \n",
    "y_texts = df['target_text'].tolist()\n",
    "y = tokenizer(y_texts, padding='max_length', max_length=150, truncation=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "# Prepare decoder input/output data\n",
    "decoder_input_data = torch.roll(y, shifts=1, dims=1)\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(tokenizer.additional_special_tokens[0])  # <start>\n",
    "decoder_input_data[:, 0] = start_token_id\n",
    "decoder_output_data = y\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "batch_size = 32\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = QADataset(X, decoder_input_data, decoder_output_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create model\n",
    "model = Seq2Seq(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_model(model, dataloader, vocab_size, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(model, tokenizer, encoder_input, encoder_outputs, hidden, cell, coverage, beam_width=3, max_len=150, length_penalty=0.8):\n",
    "    # Get token IDs for special tokens\n",
    "    start_token_id = tokenizer.convert_tokens_to_ids(\"<start>\")\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(\"<end>\")\n",
    "    \n",
    "    # Initialize sequences with start token\n",
    "    sequences = [([start_token_id], 0.0, hidden, cell, coverage, set())]  # Add empty n-gram set to each sequence\n",
    "    \n",
    "    n = 2  # Block 2-grams\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = [] \n",
    "\n",
    "        for seq, score, hidden, cell, coverage, n_grams in sequences:\n",
    "            decoder_input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)\n",
    "            output, hidden, cell, coverage = model.decoder(decoder_input, hidden, cell, encoder_outputs, coverage)\n",
    "\n",
    "            # Get top-k tokens\n",
    "            top_k_probs, top_k_indices = torch.topk(output[0, -1], beam_width * 2)  # Get more candidates to allow for filtering\n",
    "\n",
    "            for i in range(len(top_k_indices)):\n",
    "                token = top_k_indices[i].item()\n",
    "                \n",
    "                # Only check n-grams if we have enough tokens\n",
    "                if len(seq) >= n-1:\n",
    "                    current_n_gram = tuple(seq[-(n-1):] + [token])\n",
    "                    # Skip this token if the n-gram is already in this sequence's set\n",
    "                    if current_n_gram in n_grams:\n",
    "                        continue\n",
    "                    # Create a new n-gram set for this candidate by copying the current one\n",
    "                    new_n_grams = n_grams.copy()\n",
    "                    new_n_grams.add(current_n_gram)\n",
    "                else:\n",
    "                    new_n_grams = n_grams.copy()\n",
    "                \n",
    "                candidate_score = score + torch.log(top_k_probs[i]).item() / (len(seq) ** length_penalty)\n",
    "                all_candidates.append((seq + [token], candidate_score, hidden, cell, coverage, new_n_grams))\n",
    "                \n",
    "                # Break if we have enough candidates\n",
    "                if len(all_candidates) >= beam_width * 2:\n",
    "                    break\n",
    "\n",
    "        # Sort candidates and keep top beam_width\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if any sequence has generated the end token\n",
    "        if any(end_token_id in seq for seq, _, _, _, _, _ in sequences):\n",
    "            break\n",
    "\n",
    "    # Get the best sequence\n",
    "    final_seq = sequences[0][0]\n",
    "    \n",
    "    # Truncate sequence at end token if present\n",
    "    if end_token_id in final_seq:\n",
    "        final_seq = final_seq[:final_seq.index(end_token_id)]\n",
    "    \n",
    "    # Remove start token if present\n",
    "    if start_token_id in final_seq:\n",
    "        final_seq = [token for token in final_seq if token != start_token_id]\n",
    "    \n",
    "    # Decode tokens to text using the tokenizer's built-in decoder\n",
    "    return tokenizer.decode(final_seq, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(model, tokenizer, question, beam_width=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_text = f\"question: {question}\"\n",
    "        \n",
    "        # Encode input using the tokenizer\n",
    "        encodings = tokenizer(input_text, \n",
    "                              return_tensors='pt', \n",
    "                              padding='max_length',\n",
    "                              truncation=True,\n",
    "                              max_length=512).to(device)\n",
    "        \n",
    "        encoder_input = encodings['input_ids']\n",
    "        \n",
    "        # Forward pass through encoder\n",
    "        encoder_outputs, hidden, cell = model.encoder(encoder_input)\n",
    "        coverage = torch.zeros_like(encoder_outputs).to(device)\n",
    "\n",
    "        # Beam search decoding\n",
    "        return beam_search_decoder(model, tokenizer, encoder_input, encoder_outputs, hidden, cell, coverage, beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strength training is crucial for optimize hypertrophy training, as it enhances focus and cardiovascular health, but it can also be dangerous. it also induces neuroplasticity, which can be beneficial.\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "print(predict_answer(model, tokenizer, \"What are some good strength training exercises? for optimizing hypertrophy?\", beam_width=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'seq2seq_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
