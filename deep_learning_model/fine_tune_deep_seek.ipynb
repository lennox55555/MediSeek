{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: DO NOT RUN THIS LOCALLY. THIS IS JUST FOR COLAB!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will implement code to fine-tune DeepSeek on Huberman Lab podcast episodes transcripts. Colab is used for GPU access since the fine-tuning process was going to take +1 week on CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Click on Runtime -> Change runtime type -> select a gpu (T4 or A100 if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Installation and creating data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/qa_pairs/huberman_lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload all of the training data found under ```qa_pairs/huberman_lab``` (since we are only using huberman_lab) at this point (in colab you can clik on the three dots in the file explorer under ```data/qa_pairs/huberman_lab``` and then click upload and select all of the json files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft torch pandas tqdm numpy accelerate bitsandbytes\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Running the code as is from the script\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DeepSeekQAFinetuner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"deepseek-ai/deepseek-llm-1.3b-base\",\n",
    "        data_dir=\"./data/qa_pairs/huberman_lab\",\n",
    "        output_dir=\"./fine_tuned_models\",\n",
    "        device=None,\n",
    "        load_in_8bit=False,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        max_seq_length=1024,\n",
    "        batch_size=4,\n",
    "        gradient_accumulation_steps=16,\n",
    "        epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        warmup_ratio=0.03,\n",
    "    ):\n",
    "        # Set device\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Model configuration\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.load_in_8bit = load_in_8bit\n",
    "        self.lora_r = lora_r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_steps = save_steps\n",
    "        self.eval_steps = eval_steps\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Load DeepSeek model and tokenizer\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading model and tokenizer: {self.model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Add special tokens for instruction tuning if not present\n",
    "        special_tokens = {\"pad_token\": \"<pad>\", \"eos_token\": \"</s>\", \"bos_token\": \"<s>\"}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # Load model with quantization if enabled\n",
    "        if self.load_in_8bit:\n",
    "            logger.info(\"Loading model in 8-bit quantization\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                load_in_8bit=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                trust_remote_code=True\n",
    "            ).to(self.device)\n",
    "        \n",
    "        # Configure LoRA\n",
    "        logger.info(\"Applying LoRA adapters\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=self.lora_r,\n",
    "            lora_alpha=self.lora_alpha,\n",
    "            lora_dropout=self.lora_dropout,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        return self.model, self.tokenizer\n",
    "    \n",
    "    def load_and_process_data(self, val_split=0.05):\n",
    "        \"\"\"\n",
    "        Load QA pairs from JSON files and format them for instruction tuning\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading data from: {self.data_dir}\")\n",
    "        all_data = []\n",
    "        \n",
    "        # Process all JSON files\n",
    "        for file_name in os.listdir(self.data_dir):\n",
    "            if file_name.endswith('.json'):\n",
    "                file_path = os.path.join(self.data_dir, file_name)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        data = json.load(file)\n",
    "                    \n",
    "                    if not data['qa_pairs']:\n",
    "                        continue\n",
    "                        \n",
    "                    qa_pairs = data['qa_pairs']\n",
    "                    df = pd.DataFrame(qa_pairs)\n",
    "                    all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "        \n",
    "        # Combine all data\n",
    "        if not all_data:\n",
    "            raise ValueError(\"No valid data found in the specified directory\")\n",
    "            \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        logger.info(f\"Loaded {len(combined_df)} question-answer pairs\")\n",
    "        \n",
    "        # Format data for instruction tuning\n",
    "        formatted_data = []\n",
    "        for _, row in combined_df.iterrows():\n",
    "            # Format as instruction: question, response: answer\n",
    "            formatted_text = f\"<s>Human: {row['question']}\\n\\nAssistant: {row['answer']}</s>\"\n",
    "            formatted_data.append(formatted_text)\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(formatted_data))\n",
    "        val_size = int(len(formatted_data) * val_split)\n",
    "        train_indices = indices[val_size:]\n",
    "        val_indices = indices[:val_size]\n",
    "        \n",
    "        train_data = [formatted_data[i] for i in train_indices]\n",
    "        val_data = [formatted_data[i] for i in val_indices]\n",
    "        \n",
    "        logger.info(f\"Split data into {len(train_data)} training and {len(val_data)} validation examples\")\n",
    "        \n",
    "        # Tokenize data\n",
    "        logger.info(\"Tokenizing data\")\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # Create tokenized datasets\n",
    "        train_encodings = tokenize_function(train_data)\n",
    "        val_encodings = tokenize_function(val_data)\n",
    "        \n",
    "        class TextDataset(Dataset):\n",
    "            def __init__(self, encodings):\n",
    "                self.encodings = encodings\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.encodings.input_ids)\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "                item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "                return item\n",
    "        \n",
    "        train_dataset = TextDataset(train_encodings)\n",
    "        val_dataset = TextDataset(val_encodings)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Fine-tune the model on the QA dataset\n",
    "        \"\"\"\n",
    "        # Load model and tokenizer if not already loaded\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.load_model_and_tokenizer()\n",
    "        \n",
    "        # Load and process data\n",
    "        train_dataset, val_dataset = self.load_and_process_data()\n",
    "        \n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=self.epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
    "            learning_rate=self.learning_rate,\n",
    "            warmup_ratio=self.warmup_ratio,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f\"{self.output_dir}/logs\",\n",
    "            logging_steps=10,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=self.eval_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=self.save_steps,\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"tensorboard\",\n",
    "            fp16=self.device == \"cuda\",\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Set up data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        logger.info(\"Starting training\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        self.save_model()\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def save_model(self, model_path=None):\n",
    "        \"\"\"\n",
    "        Save the fine-tuned model\n",
    "        \"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = os.path.join(self.output_dir, \"final_model\")\n",
    "        \n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        logger.info(f\"Saving model to {model_path}\")\n",
    "        \n",
    "        # Save the model's PEFT adapters\n",
    "        self.model.save_pretrained(model_path)\n",
    "        \n",
    "        # Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(model_path)\n",
    "        \n",
    "        # Save in PyTorch format for compatibility with the traditional model\n",
    "        torch_path = os.path.join(model_path, \"pytorch_model.pth\")\n",
    "        torch.save(self.model.state_dict(), torch_path)\n",
    "        logger.info(f\"Model state dict saved to {torch_path}\")\n",
    "        \n",
    "        return model_path\n",
    "    \n",
    "    def generate_response(self, question, max_new_tokens=150, temperature=0.3):\n",
    "        \"\"\"\n",
    "        Generate a response for a given question using the fine-tuned model\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        prompt = f\"<s>Human: {question}\\n\\nAssistant:\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate a response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode the response\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the assistant's response\n",
    "        response = full_response.split(\"Assistant:\")[1].strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "def main():\n",
    "    # Initialize and train DeepSeek model\n",
    "    finetuner = DeepSeekQAFinetuner(\n",
    "        model_name=\"deepseek-ai/deepseek-coder-1.3b-base\",\n",
    "        data_dir=\"./data/qa_pairs/huberman_lab\",\n",
    "        output_dir=\"./models/fine_tuned_deepseek\",\n",
    "        batch_size=4,  # Adjust based on your GPU memory\n",
    "        epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        load_in_8bit=True  # Set to True if you have limited GPU memory\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer = finetuner.train()\n",
    "    \n",
    "    # Test the model with a sample question\n",
    "    question = \"How can omega-3 fatty acids benefit brain health?\"\n",
    "    response = finetuner.generate_response(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Zipping and downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r fine_tuned_models.zip models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the zipping process was done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh fine_tuned_models.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('fine_tuned_models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: We are downloading all of the files including all checkpoints in case we might need to use any of them. If you would like to download the files pertaining to the file fine-tuned model, those can be found under ```models/fine_tuned_deepseel/final_model/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
